{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5URQFfSEVn67"
      },
      "source": [
        "# Transformer Encoder\n",
        "\n",
        "This is tutorial is based on [Tutorial 6: Transformers and Multi-Head Attention](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html) of the UvA MSc AI course Deep Learning 1.\n",
        "\n",
        "**TODO**\n",
        "* Open in Colab link\n",
        "* Training and validation curves\n",
        "\n",
        "\n",
        "**Credit:** The original tutorial was developed by Phillip Lippe. Wilker Aziz modified it to give it a focus on NLP applications."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence labelling\n",
        "\n",
        "In sequence labelling we have two sequences of equal length: a word sequence $x_{1:l}$ and a tag sequence $y_{1:l}$.\n",
        "\n",
        "\n",
        "The word sequence $x_{1:l} = \\langle x_1, \\ldots, x_l \\rangle$, where $l$ is the sequence length, is such that token $x_i$ belongs to a vocabulary $\\mathcal W$ of $V$ known words. \n",
        "\n",
        "The tag sequence $y_{1:l} = \\langle y_1, \\ldots, y_l \\rangle$, where $l$ is the same length as the document $x_{1:l}$, is such that each tag $y_i$ belongs to a vocabulary $\\mathcal T$ of $C$ known tags.\n",
        "\n",
        "As always, our models predict probability distributions, thus, for each position $i$ of $x_{1:l}$, we want to predict a distribution over the $C$ possible tags. A common aspect of most sequence labellers is that we can condition on the document $x_{1:l}$ fully, since we are not interested in learning to generate it (rather, it will always be given to us). As for the tag sequence, it depends, we may want to model the tag independently of one another given $x_{1:l}$, we may want to use some history of previous tags (as in a Markov model), we may want to use all of the already generated tags (as in an autoregressive model). We may even want to use some future tags (leading to a class of models class *conditional random fields*, this class of models is not covered in this course, but it is covered in the 3rd year course on structure prediction).\n",
        "\n",
        "In this tutorial we focus on the first option, where we condition on $x_{1:l}$ and model the tags in $y_{1:l}$ independently. \n",
        "\n",
        "Statistically, we will condition on the complete input sequence and on the fact that we are tagging the $i$th word, and predict a Categorical distribution for the $i$th tag:\n",
        "\n",
        "\\begin{equation}\n",
        "    Y_i | X_{1:l} = x_{1:l}, I=i \\sim \\mathrm{Categorical}(\\mathbf f(x_{1:l}, i; \\theta))\n",
        "\\end{equation}\n",
        "\n",
        "The $C$-dimensional probability vector $\\mathbf f(x_{1:l}, i; \\theta)$ that parameterises the Categorical distribution is the output of a neural network whose trainable parameters are denoted by $\\theta$.\n",
        "\n",
        "This will be an excellent playground for introducing the **Transformer encoder**. We will use it to represent $x_{1:l}$ as $l$ hidden states, from each such hidden state we will predict the Categorical parameter using a simple feed-forward neural net with softmax output.\n",
        "\n",
        "\n",
        "Before digging into the Transformer, let's have a look at the data we will be modelling, and, on the way, let's create the necessary objects to model it using PyTorch.\n",
        "\n"
      ],
      "metadata": {
        "id": "rfu8Vx39X_yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import treebank, brown\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install nltk\n",
        "!pip install tabulate\n",
        "\n",
        "# Download some data\n",
        "nltk.download('treebank')\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "nltk.download('universal_tagset')\n"
      ],
      "metadata": {
        "id": "Ico8OkbkgpQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task and data"
      ],
      "metadata": {
        "id": "njtOUrkbmbdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be working with [part-of-speech (POS) tagging]](https://en.wikipedia.org/wiki/Part-of-speech_tagging) for English. In POS tagging, we annotate each token of a piece of text with its *part of speech category* (a form of syntactic categorisation of words). \n",
        "\n",
        "We will use an NLTK corpus of English text annotated with such level of linguistic information.  "
      ],
      "metadata": {
        "id": "g5zPoriJkWg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = brown  # If you do not have access to a GPU, you might want to use `treebank` instead (it's much smaller)\n",
        "\n",
        "def split_nltk_corpus(corpus, max_length=30, num_heldout=100):\n",
        "    \"\"\"\n",
        "    Shuffle and split a corpus.\n",
        "    corpus: a corpus of tagged sequences, each sequence is a pair, each pair is a token and a tag.\n",
        "    max_length: discard sentences longer than this\n",
        "\n",
        "    Return: \n",
        "        (training word sequences, training tag sequences), \n",
        "        (dev word sequences, dev tag sequences), \n",
        "        (test word sequences, test tag sequences),         \n",
        "    \"\"\"\n",
        "    tagged_sentences = corpus.tagged_sents(tagset='universal')\n",
        "    # do not change the seed in here    \n",
        "    order = np.random.RandomState(42).permutation(np.arange(len(tagged_sentences)))    \n",
        "    word_sequences = [[w.lower() for w, t in tagged_sentences[i]] for i in order if len(tagged_sentences[i]) <= max_length]    \n",
        "    tag_sequences = [[t for w, t in tagged_sentences[i]] for i in order if len(tagged_sentences[i]) <= max_length]    \n",
        "    return (word_sequences[2*num_heldout:], tag_sequences[2*num_heldout:]), (word_sequences[num_heldout:2*num_heldout], tag_sequences[num_heldout:2*num_heldout]), (word_sequences[:num_heldout], tag_sequences[:num_heldout])"
      ],
      "metadata": {
        "id": "c8wsYNqqWEAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some code to help us visualise the data\n",
        "\n",
        "from tabulate import tabulate\n",
        "\n",
        "def tostring(seq_pair, vertical=True, headers=['Word', 'Tag']):\n",
        "    \"\"\"\n",
        "    A sequence of pairs, each pair is a token and a tag. Use vertical=True for tabulate.\n",
        "    Return a string representing the sequence of pairs.\n",
        "    \"\"\"\n",
        "    if vertical:\n",
        "        return tabulate(list(seq_pair), headers=headers)\n",
        "    else:\n",
        "        return ' '.join(f'{w}/{t}' for w, t in seq_pair)\n",
        "\n",
        "def tostring2(tok_seq, tag_seq, vertical=True, headers=['Word', 'Tag']):\n",
        "    \"\"\"\n",
        "    A sequence of pairs, each pair is a token and a tag. Use vertical=True for tabulate.\n",
        "    Return a string representing the sequence of pairs.\n",
        "    \"\"\"\n",
        "    return tostring(zip(tok_seq, tag_seq), vertical=vertical, headers=headers)\n"
      ],
      "metadata": {
        "id": "MKjPasz8kcgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For `brown` this could take a minute."
      ],
      "metadata": {
        "id": "vYFJfoLVg3uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "(training_x, training_y), (dev_x, dev_y), (test_x, test_y) = split_nltk_corpus(corpus, num_heldout=100 if corpus is treebank else 1000)\n",
        "print(f\"Number of sentences: training={len(training_x)} dev={len(dev_x)} test={len(test_x)}\")\n",
        "\n",
        "print(\"# A few training sentences\\n\\n\")\n",
        "for n in range(3):    \n",
        "    print(tostring2(training_x[n], training_y[n]))\n",
        "    print()"
      ],
      "metadata": {
        "id": "NWHR8uVYWQBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary\n",
        "\n",
        "Now let us create a class to manage the vocabulary of *words* and the vocabulary of *tags* relevant to our dataset."
      ],
      "metadata": {
        "id": "DLu3xfQFkh5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "class Vocab:\n",
        "\n",
        "    def __init__(self, corpus: list, min_freq=1):        \n",
        "        \"\"\"\n",
        "        corpus: list of documents, each document is a list of tokens, each token is a string\n",
        "        min_freq: words that occur less than this value are discarded\n",
        "        \"\"\"\n",
        "        # Make the vocabulary of known words\n",
        "\n",
        "        # Count word occurrences\n",
        "        counter = Counter(chain(*corpus))\n",
        "        # sort them by frequency\n",
        "        sorted_by_freq_tuples = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n",
        "        \n",
        "        # Special tokens\n",
        "        self.pad_token = \"-PAD-\"        \n",
        "        self.bos_token = \"-BOS-\"\n",
        "        self.eos_token = \"-EOS-\"\n",
        "        self.unk_token = \"-UNK-\"\n",
        "        self.pad_id = 0\n",
        "        self.bos_id = 1\n",
        "        self.eos_id = 2\n",
        "        self.unk_id = 3\n",
        "\n",
        "        self.known_symbols = [self.pad_token, self.bos_token, self.eos_token, self.unk_token]\n",
        "        self.counts = [0, 0]\n",
        "        \n",
        "        # Vocabulary\n",
        "        self.word2id = OrderedDict()                \n",
        "        self.word2id[self.pad_token] = self.pad_id        \n",
        "        self.word2id[self.bos_token] = self.bos_id\n",
        "        self.word2id[self.eos_token] = self.eos_id\n",
        "        self.word2id[self.unk_token] = self.unk_id\n",
        "        self.min_freq = min_freq\n",
        "        for w, n in sorted_by_freq_tuples: \n",
        "            if n >= min_freq: # discard infrequent words\n",
        "                self.word2id[w] = len(self.known_symbols)\n",
        "                self.known_symbols.append(w)\n",
        "                self.counts.append(n)\n",
        "        \n",
        "        # store the counts for later\n",
        "        self.counts = np.array(self.counts)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.known_symbols)\n",
        "\n",
        "    def __getitem__(self, word): # return the id (int) of a word (str)\n",
        "        return self.word2id.get(word, self.unk_id)\n",
        "\n",
        "    def encode(self, doc: list, add_bos=False, add_eos=False, pad_right=0):\n",
        "        \"\"\"\n",
        "        Transform a document into a numpy array of integer token identifiers.\n",
        "        doc: list of tokens, each token is a string\n",
        "        add_bos: whether to add the BOS token\n",
        "        add_eos: whether to add the EOS token\n",
        "        pad_right: number of suffix padding tokens \n",
        "        \n",
        "        Return: a list of codes (possibly with BOS and EOS added as well as padding)\n",
        "        \"\"\"\n",
        "        return [self.word2id.get(w, self.unk_id) for w in chain([self.bos_token] * int(add_bos), doc, [self.eos_token] * int(add_eos), [self.pad_token] * pad_right)]\n",
        "\n",
        "    def batch_encode(self, docs: list, add_bos=False, add_eos=False):\n",
        "        \"\"\"\n",
        "        Transform a batch of documents into a numpy array of integer token identifiers.\n",
        "        This will pad the shorter documents to the length of the longest document.\n",
        "        docs: a list of documents\n",
        "        add_bos: whether to add the BOS token\n",
        "        add_eos: whether to add the EOS token\n",
        "        pad_right: number of suffix padding tokens\n",
        "\n",
        "        Return: numpy array with shape [len(docs), longest_doc + add_bos + add_eos]\n",
        "        \"\"\"\n",
        "        max_len = max(len(doc) for doc in docs)\n",
        "        return np.array([self.encode(doc, add_bos=add_bos, add_eos=add_eos, pad_right=max_len-len(doc)) for doc in docs])\n",
        "\n",
        "    def decode(self, ids, strip_pad=False):\n",
        "        \"\"\"\n",
        "        Transform a np.array document into a list of tokens.\n",
        "        ids: np.array with shape [num_tokens] \n",
        "        strip_pad: whether PAD tokens should be deleted from the output\n",
        "\n",
        "        Return: list of strings with size [num_tokens - num_padding]\n",
        "        \"\"\"\n",
        "        if strip_pad:\n",
        "            return [self.known_symbols[id] for id in ids if id != self.pad_id]\n",
        "        else:\n",
        "            return [self.known_symbols[id] for id in ids]\n",
        "\n",
        "    def batch_decode(self, docs, strip_pad=False):\n",
        "        \"\"\"\n",
        "        Transform a np.array collection of documents into a collection of lists of tokens.\n",
        "        ids: np.array with shape [num_docs, max_length] \n",
        "        strip_pad: whether PAD tokens should be deleted from the output\n",
        "\n",
        "        Return: list of documents, each a list of tokens, each token a string\n",
        "        \"\"\"\n",
        "        return [self.decode(doc, strip_pad=strip_pad) for doc in docs]    "
      ],
      "metadata": {
        "id": "N8ckVFEjWaPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See what this does"
      ],
      "metadata": {
        "id": "JQZ_FftxkpY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we get a vocabulary for words\n",
        "word_vocab = Vocab(training_x, min_freq=1)\n",
        "# and a vocabulary for tags\n",
        "tag_vocab = Vocab(training_y, min_freq=1)\n",
        "# you can see their sizes V and C:\n",
        "len(word_vocab), len(tag_vocab)"
      ],
      "metadata": {
        "id": "NHFnTt6eWgEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `encode` method turns a sequence of (str) symbols into a sequence of (int) codes:"
      ],
      "metadata": {
        "id": "1U9gRCpplPas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tostring2(word_vocab.encode(training_x[0]), tag_vocab.encode(training_y[0]), vertical=True))"
      ],
      "metadata": {
        "id": "BPqpXKTrksvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also have `encode` add some special symbols for us (but remember to be consistent, you should always have token sequences and tag sequences that match in length):"
      ],
      "metadata": {
        "id": "soU1IcOflWwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tostring2(word_vocab.encode(training_x[0], add_eos=True), tag_vocab.encode(training_y[0], add_eos=True)))"
      ],
      "metadata": {
        "id": "6PXLBI7dlZLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here for example, we will add BOS, EOS, and we are going to encode and decode:"
      ],
      "metadata": {
        "id": "H_uZJ2nylbvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tostring2(word_vocab.decode(word_vocab.encode(training_x[0], add_bos=True, add_eos=True)), tag_vocab.decode(tag_vocab.encode(training_y[0], add_bos=True, add_eos=True))))"
      ],
      "metadata": {
        "id": "r_zmklnPleVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also encode and decode entire batches of sequences. This will use pad symbols/codes to make the sequences in the same batch have the same length:"
      ],
      "metadata": {
        "id": "UOrU7UFPlhWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vocab.batch_encode(training_x[:2], add_bos=False, add_eos=True)"
      ],
      "metadata": {
        "id": "3QJehMBgllIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vocab.batch_decode(word_vocab.batch_encode(training_x[:2], add_bos=False, add_eos=True), strip_pad=True)"
      ],
      "metadata": {
        "id": "FXZNCI8KlnI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch Dataset and Dataloader"
      ],
      "metadata": {
        "id": "RhvoJmkGlkcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TaggedCorpus(Dataset):\n",
        "    \"\"\"\n",
        "    Use this to give torch access to a corpus of tagged sequences.\n",
        "    This class will also know the vocab objects for tokens and tags, \n",
        "    and it will take care of coding strings into integers consistently.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, corpus_x, corpus_y, word_vocab: Vocab, tag_vocab: Vocab):\n",
        "        \"\"\"\n",
        "        In PyTorch we better always manipulate numerical codes, rather than text.\n",
        "        So, our Corpus object will contain a vocab that converts words to codes.\n",
        "\n",
        "        corpus_x: token sequences\n",
        "        corpus_y: tag sequences\n",
        "        word_vocab: vocabulary for token sequences\n",
        "        tag_vocab: vocabulary for tag sequences\n",
        "        \"\"\"\n",
        "        self.corpus_x = list(corpus_x)\n",
        "        self.corpus_y = list(corpus_y)\n",
        "        assert len(self.corpus_x) == len(self.corpus_y), \"I need sequence pairs\"\n",
        "        assert all(len(x) == len(y) for x, y in zip(corpus_x, corpus_y)), \"A sequence pair should match in number of steps\"\n",
        "        self.word_vocab = word_vocab\n",
        "        self.tag_vocab = tag_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Size of the corpus in number of sequence pairs\"\"\"\n",
        "        return len(self.corpus_x)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return corpus_x[idx] and corpus_y[idx] converted to codes and with the EOS code in the end\"\"\"\n",
        "        x = self.word_vocab.encode(self.corpus_x[idx], add_bos=False, add_eos=True)\n",
        "        y = self.tag_vocab.encode(self.corpus_y[idx], add_bos=False, add_eos=True)\n",
        "        return x, y\n",
        "\n",
        "    @classmethod\n",
        "    def pad_to_longest(cls, pairs, pad_id=0):\n",
        "        \"\"\"\n",
        "        Take a list of coded sequences and returns a torch tensor where \n",
        "        every sentence has the same length (by means of using PAD tokens)\n",
        "        \"\"\"\n",
        "        longest = max(len(x) for x, y in pairs)\n",
        "        batch_x = torch.tensor([x + [pad_id] * (longest - len(x)) for x, y in pairs]) \n",
        "        batch_y = torch.tensor([y + [pad_id] * (longest - len(y)) for x, y in pairs]) \n",
        "        return batch_x, batch_y"
      ],
      "metadata": {
        "id": "B3-z3Qu-WlX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how to use this"
      ],
      "metadata": {
        "id": "fcIOh5Zrm0x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training = TaggedCorpus(training_x, training_y, word_vocab, tag_vocab)\n",
        "dev = TaggedCorpus(dev_x, dev_y, word_vocab, tag_vocab)\n",
        "test = TaggedCorpus(test_x, test_y, word_vocab, tag_vocab)\n",
        "\n",
        "# A DataLoader is how Pytorch makes batches of data points from the datasets above\n",
        "batcher = DataLoader(training, batch_size=3, shuffle=True, collate_fn=TaggedCorpus.pad_to_longest)\n",
        "for batch_x, batch_y in batcher:\n",
        "    print(\"# This is how the sequence pairs in a batch come out of the data loader\\n\")\n",
        "\n",
        "    for x, y in zip(batch_x, batch_y):\n",
        "        print(tostring2(x, y))\n",
        "        print()\n",
        "    \n",
        "    print(\"# And we can always decode them for inspection\\n\")\n",
        "    # stripping padding makes it easier to read the examples\n",
        "    for x, y in zip(word_vocab.batch_decode(batch_x, strip_pad=True), tag_vocab.batch_decode(batch_y, strip_pad=True)):\n",
        "        print(tostring2(x, y))\n",
        "        print()\n",
        "    break"
      ],
      "metadata": {
        "id": "FLDu8z_LWn9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seting up for Transformers"
      ],
      "metadata": {
        "id": "wKfr0ZNHm3vT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8XUSxhDVn6-"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "\n",
        "**Note:** Interested in JAX? Check out our [JAX+Flax version](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial6/Transformers_and_MHAttention.html) of this tutorial!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehcUMuSlVn6_"
      },
      "source": [
        "In this tutorial, we will discuss one of the most impactful architectures of the last 2 years: the Transformer model. Since the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. had been published in 2017, the Transformer architecture has continued to beat benchmarks in many domains, most importantly in Natural Language Processing. Transformers with an incredible amount of parameters can generate long, convincing [essays](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3), and opened up new application fields of AI. As the hype of the Transformer architecture seems not to come to an end in the next years, it is important to understand how it works, and have implemented it yourself, which we will do in this notebook.\n",
        "\n",
        "Despite the huge success of Transformers in NLP, we will _not_ include the NLP domain in our notebook here. Why? Firstly, the Master AI at UvA offers many great NLP courses that will take a closer look at the application of the Transformer architecture in NLP ([NLP2](https://studiegids.uva.nl/xmlpages/page/2020-2021/zoek-vak/vak/79628), [Advanced Topics in Computational Semantics](https://studiegids.uva.nl/xmlpages/page/2020-2021/zoek-vak/vak/80162)). Secondly, assignment 2 takes already a closer look at language generation on character level, on which you could easily apply our transformer architecture. Finally, and most importantly, there is so much more to the Transformer architecture. NLP is the domain the Transformer architecture has been originally proposed for and had the greatest impact on, but it also accelerated research in other domains, recently even [Computer Vision](https://arxiv.org/abs/2010.11929). Thus, we focus here on what makes the Transformer and self-attention so powerful in general. In [Tutorial 15](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html), we will discuss the application of Transformers in Computer Vision.\n",
        "\n",
        "Below, we import our standard libraries. Similarly as in Tutorial 5, we will use [PyTorch Lightning](https://www.pytorchlightning.ai/) as an additional framework. If you are not familiar with PyTorch Lightning, please make sure to have read Tutorial 5 carefully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F1W2L2eVn6_"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import numpy as np \n",
        "import random\n",
        "import math\n",
        "import json\n",
        "from functools import partial\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline \n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "## tqdm for loading bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision import transforms\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/transformers\"\n",
        "\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EChcug-Vn7C"
      },
      "source": [
        "## The Transformer architecture\n",
        "\n",
        "In the first part of this notebook, we will implement the Transformer architecture by hand. As the architecture is so popular, there already exists a Pytorch module `nn.Transformer` ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)) and a [tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) on how to use it for next token prediction. However, we will implement it here ourselves, to get through to the smallest details.\n",
        "\n",
        "There are of course many more tutorials out there about attention and Transformers. Below, we list a few that are worth exploring if you are interested in the topic and might want yet another perspective on the topic after this one:\n",
        "\n",
        "* [Transformer: A Novel Neural Network Architecture for Language Understanding (Jakob Uszkoreit, 2017)](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) - The original Google blog post about the Transformer paper, focusing on the application in machine translation.\n",
        "* [The Illustrated Transformer (Jay Alammar, 2018)](http://jalammar.github.io/illustrated-transformer/) - A very popular and great blog post intuitively explaining the Transformer architecture with many nice visualizations. The focus is on NLP.\n",
        "* [Attention? Attention! (Lilian Weng, 2018)](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) - A nice blog post summarizing attention mechanisms in many domains including vision.\n",
        "* [Illustrated: Self-Attention (Raimi Karim, 2019)](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a) - A nice visualization of the steps of self-attention. Recommended going through if the explanation below is too abstract for you.\n",
        "* [The Transformer family (Lilian Weng, 2020)](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html) - A very detailed blog post reviewing more variants of Transformers besides the original one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tba-JbW9Vn7C"
      },
      "source": [
        "### What is Attention?\n",
        "\n",
        "The attention mechanism describes a recent new group of layers in neural networks that has attracted a lot of interest in the past few years, especially in sequence tasks. There are a lot of different possible definitions of \"attention\" in the literature, but the one we will use here is the following: _the attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements' keys_. So what does this exactly mean? The goal is to take an average over the features of multiple elements. However, instead of weighting each element equally, we want to weight them depending on their actual values. In other words, we want to dynamically decide on which inputs we want to \"attend\" more than others. In particular, an attention mechanism has usually four parts we need to specify:\n",
        "\n",
        "* **Query**: The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n",
        "* **Keys**: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is \"offering\", or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n",
        "* **Values**: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n",
        "* **Score function**: To rate which elements we want to pay attention to, we need to specify a score function $f_{attn}$. The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\n",
        "\n",
        "\n",
        "The weights of the average are calculated by a softmax over all score function outputs. Hence, we assign those value vectors a higher weight whose corresponding key is most similar to the query. If we try to describe it with pseudo-math, we can write: \n",
        "\n",
        "$$\n",
        "\\alpha_i = \\frac{\\exp\\left(f_{attn}\\left(\\text{key}_i, \\text{query}\\right)\\right)}{\\sum_j \\exp\\left(f_{attn}\\left(\\text{key}_j, \\text{query}\\right)\\right)}, \\hspace{5mm} \\text{out} = \\sum_i \\alpha_i \\cdot \\text{value}_i\n",
        "$$\n",
        "\n",
        "Visually, we can show the attention over a sequence of words as follows:\n",
        "\n",
        "<center width=\"100%\" style=\"padding:25px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/attention_example.svg?raw=1\" width=\"750px\"></center>\n",
        "\n",
        "For every word, we have one key and one value vector. The query is compared to all keys with a score function (in this case the dot product) to determine the weights. The softmax is not visualized for simplicity. Finally, the value vectors of all words are averaged using the attention weights.\n",
        "\n",
        "Most attention mechanisms differ in terms of what queries they use, how the key and value vectors are defined, and what score function is used. The attention applied inside the Transformer architecture is called **self-attention**. In self-attention, each sequence element provides a key, value, and query. For each element, we perform an attention layer where based on its query, we check the similarity of the all sequence elements' keys, and returned a different, averaged value vector for each element. We will now go into a bit more detail by first looking at the specific implementation of the attention mechanism which is in the Transformer case the scaled dot product attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2MnU_ptVn7D"
      },
      "source": [
        "### Scaled Dot Product Attention\n",
        "\n",
        "The core concept behind self-attention is the scaled dot product attention. Our goal is to have an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute. The dot product attention takes as input a set of queries $Q\\in\\mathbb{R}^{T\\times d_k}$, keys $K\\in\\mathbb{R}^{T\\times d_k}$ and values $V\\in\\mathbb{R}^{T\\times d_v}$ where $T$ is the sequence length, and $d_k$ and $d_v$ are the hidden dimensionality for queries/keys and values respectively. For simplicity, we neglect the batch dimension for now. The attention value from element $i$ to $j$ is based on its similarity of the query $Q_i$ and key $K_j$, using the dot product as the similarity metric. In math, we calculate the dot product attention as follows:\n",
        "\n",
        "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "The matrix multiplication $QK^T$ performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape $T\\times T$. Each row represents the attention logits for a specific element $i$ to all other elements in the sequence. On these, we apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention). Another perspective on this attention mechanism offers the computation graph which is visualized below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/scaled_dot_product_attn.svg?raw=1\" width=\"210px\"></center>\n",
        "\n",
        "One aspect we haven't discussed yet is the scaling factor of $1/\\sqrt{d_k}$. This scaling factor is crucial to maintain an appropriate variance of attention values after initialization. Remember that we intialize our layers with the intention of having equal variance throughout the model, and hence, $Q$ and $K$ might also have a variance close to $1$. However, performing a dot product over two vectors with a variance $\\sigma^2$ results in a scalar having $d_k$-times higher variance: \n",
        "\n",
        "$$q_i \\sim \\mathcal{N}(0,\\sigma^2), k_i \\sim \\mathcal{N}(0,\\sigma^2) \\to \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i\\cdot k_i\\right) = \\sigma^4\\cdot d_k$$\n",
        "\n",
        "\n",
        "If we do not scale down the variance back to $\\sim\\sigma^2$, the softmax over the logits will already saturate to $1$ for one random element and $0$ for all others. The gradients through the softmax will be close to zero so that we can't learn the parameters appropriately. Note that the extra factor of $\\sigma^2$, i.e., having $\\sigma^4$ instead of $\\sigma^2$, is usually not an issue, since we keep the original variance $\\sigma^2$ close to $1$ anyways.\n",
        "\n",
        "The block `Mask (opt.)` in the diagram above represents the optional masking of specific entries in the attention matrix. This is for instance used if we stack multiple sequences with different lengths into a batch. To still benefit from parallelization in PyTorch, we pad the sentences to the same length and mask out the padding tokens during the calculation of the attention values. This is usually done by setting the respective attention logits to a very low value. \n",
        "\n",
        "After we have discussed the details of the scaled dot product attention block, we can write a function below which computes the output features given the triple of queries, keys, and values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpXsoT7pVn7D"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attn_logits = attn_logits / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "    attention = F.softmax(attn_logits, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arxm9dHiVn7D"
      },
      "source": [
        "Note that our code above supports any additional dimensionality in front of the sequence length so that we can also use it for batches. However, for a better understanding, let's generate a few random queries, keys, and value vectors, and calculate the attention outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rZIEIKFVn7E"
      },
      "outputs": [],
      "source": [
        "seq_len, d_k = 3, 2\n",
        "pl.seed_everything(42)\n",
        "q = torch.randn(seq_len, d_k)\n",
        "k = torch.randn(seq_len, d_k)\n",
        "v = torch.randn(seq_len, d_k)\n",
        "values, attention = scaled_dot_product(q, k, v)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"Values\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs_m9rDhVn7E"
      },
      "source": [
        "Before continuing, make sure you can follow the calculation of the specific values here, and also check it by hand. It is important to fully understand how the scaled dot product attention is calculated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJpnHgAaVn7E"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features. Specifically, given a query, key, and value matrix, we transform those into $h$ sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix. Mathematically, we can express this operation as:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
        "    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "We refer to this as Multi-Head Attention layer with the learnable parameters $W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}$, and $W^{O}\\in\\mathbb{R}^{h\\cdot d_v\\times d_{out}}$ ($D$ being the input dimensionality). Expressed in a computational graph, we can visualize it as below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/multihead_attention.svg?raw=1\" width=\"230px\"></center>\n",
        "\n",
        "How are we applying a Multi-Head Attention layer in a neural network, where we don't have an arbitrary query, key, and value vector as input? Looking at the computation graph above, a simple but effective implementation is to set the current feature map in a NN, $X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$, as $Q$, $K$ and $V$ ($B$ being the batch size, $T$ the sequence length, $d_{\\text{model}}$ the hidden dimensionality of $X$). The consecutive weight matrices $W^{Q}$, $W^{K}$, and $W^{V}$ can transform $X$ to the corresponding feature vectors that represent the queries, keys, and values of the input. Using this approach, we can implement the Multi-Head Attention module below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km2GB83oVn7E"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        \n",
        "        # Stack all weight matrices 1...h together for efficiency\n",
        "        # Note that in many implementations you see \"bias=False\" which is optional\n",
        "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        \n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "        qkv = self.qkv_proj(x)\n",
        "        \n",
        "        # Separate Q, K, V from linear output\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        \n",
        "        # Determine value outputs\n",
        "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
        "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
        "        o = self.o_proj(values)\n",
        "        \n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhu1KRpcVn7F"
      },
      "source": [
        "One crucial characteristic of the multi-head attention is that it is permutation-equivariant with respect to its inputs. This means that if we switch two input elements in the sequence, e.g. $X_1\\leftrightarrow X_2$ (neglecting the batch dimension for now), the output is exactly the same besides the elements 1 and 2 switched. Hence, the multi-head attention is actually looking at the input not as a sequence, but as a set of elements. This property makes the multi-head attention block and the Transformer architecture so powerful and widely applicable! But what if the order of the input is actually important for solving the task, like language modeling? The answer is to encode the position in the input features, which we will take a closer look at later (topic _Positional encodings_ below).\n",
        "\n",
        "Before moving on to creating the Transformer architecture, we can compare the self-attention operation with our other common layer competitors for sequence data: convolutions and recurrent neural networks. Below you can find a table by [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) on the complexity per layer, the number of sequential operations, and maximum path length. The complexity is measured by the upper bound of the number of operations to perform, while the maximum path length represents the maximum number of steps a forward or backward signal has to traverse to reach any other position. The lower this length, the better gradient signals can backpropagate for long-range dependencies. Let's take a look at the table below:\n",
        "\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/comparison_conv_rnn.svg?raw=1\" width=\"600px\"></center>\n",
        "\n",
        "$n$ is the sequence length, $d$ is the representation dimension and $k$ is the kernel size of convolutions. In contrast to recurrent networks, the self-attention layer can parallelize all its operations making it much faster to execute for smaller sequence lengths. However, when the sequence length exceeds the hidden dimensionality, self-attention becomes more expensive than RNNs. One way of reducing the computational cost for long sequences is by restricting the self-attention to a neighborhood of inputs to attend over, denoted by $r$. Nevertheless, there has been recently a lot of work on more efficient Transformer architectures that still allow long dependencies, of which you can find an overview in the paper by [Tay et al. (2020)](https://arxiv.org/abs/2009.06732) if interested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40UlfHm7Vn7F"
      },
      "source": [
        "### Transformer Encoder\n",
        "\n",
        "Next, we will look at how to apply the multi-head attention block inside the Transformer architecture. Originally, the Transformer model was designed for machine translation. Hence, it got an encoder-decoder structure where the encoder takes as input the sentence in the original language and generates an attention-based representation. On the other hand, the decoder attends over the encoded information and generates the translated sentence in an autoregressive manner, as in a standard RNN. While this structure is extremely useful for Sequence-to-Sequence tasks with the necessity of autoregressive decoding, we will focus here on the encoder part. Many advances in NLP have been made using pure encoder-based Transformer models (if interested, models include the [BERT](https://arxiv.org/abs/1810.04805)-family, the [Vision Transformer](https://arxiv.org/abs/2010.11929), and more), and in our tutorial, we will also mainly focus on the encoder part. If you have understood the encoder architecture, the decoder is a very small step to implement as well. The full Transformer architecture looks as follows (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/transformer_architecture.svg?raw=1\" width=\"400px\"></center>\n",
        "\n",
        "The encoder consists of $N$ identical blocks that are applied in sequence. Taking as input $x$, it is first passed through a Multi-Head Attention block as we have implemented above. The output is added to the original input using a residual connection, and we apply a consecutive Layer Normalization on the sum. Overall, it calculates $\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))$ ($x$ being $Q$, $K$ and $V$ input to the attention layer). The residual connection is crucial in the Transformer architecture for two reasons: \n",
        "\n",
        "1. Similar to ResNets, Transformers are designed to be very deep. Some models contain more than 24 blocks in the encoder. Hence, the residual connections are crucial for enabling a smooth gradient flow through the model.\n",
        "2. Without the residual connection, the information about the original sequence is lost. Remember that the Multi-Head Attention layer ignores the position of elements in a sequence, and can only learn it based on the input features. Removing the residual connections would mean that this information is lost after the first attention layer (after initialization), and with a randomly initialized query and key vector, the output vectors for position $i$ has no relation to its original input. All outputs of the attention are likely to represent similar/same information, and there is no chance for the model to distinguish which information came from which input element. An alternative option to residual connection would be to fix at least one head to focus on its original input, but this is very inefficient and does not have the benefit of the improved gradient flow.\n",
        "\n",
        "The Layer Normalization also plays an important role in the Transformer architecture as it enables faster training and provides small regularization. Additionally, it ensures that the features are in a similar magnitude among the elements in the sequence. We are not using Batch Normalization because it depends on the batch size which is often small with Transformers (they require a lot of GPU memory), and BatchNorm has shown to perform particularly bad in language as the features of words tend to have a much higher variance (there are many, very rare words which need to be considered for a good distribution estimate).\n",
        "\n",
        "Additionally to the Multi-Head Attention, a small fully connected feed-forward network is added to the model, which is applied to each position separately and identically. Specifically, the model uses a Linear$\\to$ReLU$\\to$Linear MLP. The full transformation including the residual connection can be expressed as:  \n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n",
        "    x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "This MLP adds extra complexity to the model and allows transformations on each sequence element separately. You can imagine as this allows the model to \"post-process\" the new information added by the previous Multi-Head Attention, and prepare it for the next attention block. Usually, the inner dimensionality of the MLP is 2-8$\\times$ larger than $d_{\\text{model}}$, i.e. the dimensionality of the original input $x$. The general advantage of a wider layer instead of a narrow, multi-layer MLP is the faster, parallelizable execution.\n",
        "\n",
        "Finally, after looking at all parts of the encoder architecture, we can start implementing it below. We first start by implementing a single encoder block. Additionally to the layers described above, we will add dropout layers in the MLP and on the output of the MLP and Multi-Head Attention for regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gk6CeQbVn7F"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim - Dimensionality of the input\n",
        "            num_heads - Number of heads to use in the attention block\n",
        "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
        "            dropout - Dropout probability to use in the dropout layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Attention layer\n",
        "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
        "        \n",
        "        # Two-layer MLP\n",
        "        self.linear_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, input_dim)\n",
        "        )\n",
        "        \n",
        "        # Layers to apply in between the main layers\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Attention part\n",
        "        attn_out = self.self_attn(x, mask=mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        x = self.norm1(x)\n",
        "        \n",
        "        # MLP part\n",
        "        linear_out = self.linear_net(x)\n",
        "        x = x + self.dropout(linear_out)\n",
        "        x = self.norm2(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKhz5pBRVn7G"
      },
      "source": [
        "Based on this block, we can implement a module for the full Transformer encoder. Additionally to a forward function that iterates through the sequence of encoder blocks, we also provide a function called `get_attention_maps`. The idea of this function is to return the attention probabilities for all Multi-Head Attention blocks in the encoder. This helps us in understanding, and in a sense, explaining the model. However, the attention probabilities should be interpreted with a grain of salt as it does not necessarily reflect the true interpretation of the model (there is a series of papers about this, including [Attention is not Explanation](https://arxiv.org/abs/1902.10186) and [Attention is not not Explanation](https://arxiv.org/abs/1908.04626))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7HJPsOeVn7G"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_layers, **block_args):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for l in self.layers:\n",
        "            x = l(x, mask=mask)\n",
        "        return x\n",
        "\n",
        "    def get_attention_maps(self, x, mask=None):\n",
        "        attention_maps = []\n",
        "        for l in self.layers:\n",
        "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
        "            attention_maps.append(attn_map)\n",
        "            x = l(x)\n",
        "        return attention_maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm11ghJgVn7G"
      },
      "source": [
        "### Positional encoding\n",
        "\n",
        "We have discussed before that the Multi-Head Attention block is permutation-equivariant, and cannot distinguish whether an input comes before another one in the sequence or not. In tasks like language understanding, however, the position is important for interpreting the input words. The position information can therefore be added via the input features. We could learn a embedding for every possible position, but this would not generalize to a dynamical input sequence length. Hence, the better option is to use feature patterns that the network can identify from the features and potentially generalize to larger sequences. The specific pattern chosen by Vaswani et al. are sine and cosine functions of different frequencies, as follows:\n",
        "\n",
        "$$\n",
        "PE_{(pos,i)} = \\begin{cases}\n",
        "    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n",
        "    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$PE_{(pos,i)}$ represents the position encoding at position $pos$ in the sequence, and hidden dimensionality $i$. These values, concatenated for all hidden dimensions, are added to the original input features (in the Transformer visualization above, see \"Positional encoding\"), and constitute the position information. We distinguish between even ($i \\text{ mod } 2=0$) and uneven ($i \\text{ mod } 2=1$) hidden dimensionalities where we apply a sine/cosine respectively. The intuition behind this encoding is that you can represent $PE_{(pos+k,:)}$ as a linear function of $PE_{(pos,:)}$, which might allow the model to easily attend to relative positions. The wavelengths in different dimensions range from $2\\pi$ to $10000\\cdot 2\\pi$.\n",
        "\n",
        "The positional encoding is implemented below. The code is taken from the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model) about Transformers on NLP and adjusted for our purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9v0gS_CVn7G"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            d_model - Hidden dimensionality of the input.\n",
        "            max_len - Maximum length of a sequence to expect.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        \n",
        "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
        "        # Used for tensors that need to be on the same device as the module.\n",
        "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model) \n",
        "        self.register_buffer('pe', pe, persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXm1GmwyVn7H"
      },
      "source": [
        "To understand the positional encoding, we can visualize it below. We will generate an image of the positional encoding over hidden dimensionality and position in a sequence. Each pixel, therefore, represents the change of the input feature we perform to encode the specific position. Let's do it below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shAUl15mVn7H"
      },
      "outputs": [],
      "source": [
        "encod_block = PositionalEncoding(d_model=48, max_len=96)\n",
        "pe = encod_block.pe.squeeze().T.cpu().numpy()\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\n",
        "pos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\n",
        "fig.colorbar(pos, ax=ax)\n",
        "ax.set_xlabel(\"Position in sequence\")\n",
        "ax.set_ylabel(\"Hidden dimension\")\n",
        "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
        "ax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\n",
        "ax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrPg0c08Vn7H"
      },
      "source": [
        "You can clearly see the sine and cosine waves with different wavelengths that encode the position in the hidden dimensions. Specifically, we can look at the sine/cosine wave for each hidden dimension separately, to get a better intuition of the pattern. Below we visualize the positional encoding for the hidden dimensions $1$, $2$, $3$ and $4$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uE9aCaNVn7H"
      },
      "outputs": [],
      "source": [
        "sns.set_theme()\n",
        "fig, ax = plt.subplots(2, 2, figsize=(12,4))\n",
        "ax = [a for a_list in ax for a in a_list]\n",
        "for i in range(len(ax)):\n",
        "    ax[i].plot(np.arange(1,17), pe[i,:16], color=f'C{i}', marker=\"o\", markersize=6, markeredgecolor=\"black\")\n",
        "    ax[i].set_title(f\"Encoding in hidden dimension {i+1}\")\n",
        "    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n",
        "    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n",
        "    ax[i].set_xticks(np.arange(1,17))\n",
        "    ax[i].tick_params(axis='both', which='major', labelsize=10)\n",
        "    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n",
        "    ax[i].set_ylim(-1.2, 1.2)\n",
        "fig.subplots_adjust(hspace=0.8)\n",
        "sns.reset_orig()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g6HUwxPVn7I"
      },
      "source": [
        "As we can see, the patterns between the hidden dimension $1$ and $2$ only differ in the starting angle. The wavelength is $2\\pi$, hence the repetition after position $6$. The hidden dimensions $2$ and $3$ have about twice the wavelength. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLiG42AdVn7I"
      },
      "source": [
        "### Learning rate warm-up\n",
        "\n",
        "One commonly used technique for training a Transformer is learning rate warm-up. This means that we gradually increase the learning rate from 0 on to our originally specified learning rate in the first few iterations. Thus, we slowly start learning instead of taking very large steps from the beginning. In fact, training a deep Transformer without learning rate warm-up can make the model diverge and achieve a much worse performance on training and testing. Take for instance the following plot by [Liu et al. (2019)](https://arxiv.org/pdf/1908.03265.pdf) comparing Adam-vanilla (i.e. Adam without warm-up) vs Adam with a warm-up:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/warmup_loss_plot.svg?raw=1\" width=\"350px\"></center>\n",
        "\n",
        "Clearly, the warm-up is a crucial hyperparameter in the Transformer architecture. Why is it so important? There are currently two common explanations. Firstly, Adam uses the bias correction factors which however can lead to a higher variance in the adaptive learning rate during the first iterations. Improved optimizers like [RAdam](https://arxiv.org/abs/1908.03265) have been shown to overcome this issue, not requiring warm-up for training Transformers. Secondly, the iteratively applied Layer Normalization across layers can lead to very high gradients during the first iterations, which can be solved by using [Pre-Layer Normalization](https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf) (similar to Pre-Activation ResNet), or replacing Layer Normalization by other techniques ([Adaptive Normalization](https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf), [Power Normalization](https://arxiv.org/abs/2003.07845)). \n",
        "\n",
        "Nevertheless, many applications and papers still use the original Transformer architecture with Adam, because warm-up is a simple, yet effective way of solving the gradient problem in the first iterations. There are many different schedulers we could use. For instance, the original Transformer paper used an exponential decay scheduler with a warm-up. However, the currently most popular scheduler is the cosine warm-up scheduler, which combines warm-up with a cosine-shaped learning rate decay. We can implement it below, and visualize the learning rate factor over epochs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnmh3UkjVn7I"
      },
      "outputs": [],
      "source": [
        "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
        "    \n",
        "    def __init__(self, optimizer, warmup, max_iters):\n",
        "        self.warmup = warmup\n",
        "        self.max_num_iters = max_iters\n",
        "        super().__init__(optimizer)\n",
        "        \n",
        "    def get_lr(self):\n",
        "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
        "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
        "    \n",
        "    def get_lr_factor(self, epoch):\n",
        "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
        "        if epoch <= self.warmup:\n",
        "            lr_factor *= epoch * 1.0 / self.warmup\n",
        "        return lr_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELXHgvziVn7I"
      },
      "outputs": [],
      "source": [
        "# Needed for initializing the lr scheduler\n",
        "p = nn.Parameter(torch.empty(4,4))\n",
        "optimizer = optim.Adam([p], lr=1e-3)\n",
        "lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=100, max_iters=2000)\n",
        "\n",
        "# Plotting\n",
        "epochs = list(range(2000))\n",
        "sns.set()\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(epochs, [lr_scheduler.get_lr_factor(e) for e in epochs])\n",
        "plt.ylabel(\"Learning rate factor\")\n",
        "plt.xlabel(\"Iterations (in batches)\")\n",
        "plt.title(\"Cosine Warm-up Learning Rate Scheduler\")\n",
        "plt.show()\n",
        "sns.reset_orig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzFCq4ZLVn7I"
      },
      "source": [
        "In the first 100 iterations, we increase the learning rate factor from 0 to 1, whereas for all later iterations, we decay it using the cosine wave. Pre-implementations of this scheduler can be found in the popular NLP Transformer library [huggingface](https://huggingface.co/transformers/main_classes/optimizer_schedules.html?highlight=cosine#transformers.get_cosine_schedule_with_warmup)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDbyfSo0Vn7I"
      },
      "source": [
        "### PyTorch Lightning Module\n",
        "\n",
        "Finally, we can embed the Transformer architecture into a PyTorch lightning module. From Tutorial 5, you know that PyTorch Lightning simplifies our training and test code, as well as structures the code nicely in separate functions. We will implement a template for a classifier based on the Transformer encoder. Thereby, we have a prediction output per sequence element. If we would need a classifier over the whole sequence, the common approach is to add an additional `[CLS]` token to the sequence, representing the classifier token. However, here we focus on tasks where we have an output per element. \n",
        "\n",
        "Additionally to the Transformer architecture, we add a small input network (that maps input token indices to model dimensions), the positional encoding, and an output network (transforms output encodings to predictions). We also add the learning rate scheduler, which takes a step each iteration instead of once per epoch. This is needed for the warmup and the smooth cosine decay. The training, validation, and test step is left empty for now and will be filled for our task-specific models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RVUDJi2Vn7J"
      },
      "outputs": [],
      "source": [
        "class TransformerPredictor(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, vocab_size, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, pad_id=0, ignore_target=0, dropout=0.0, input_dropout=0.0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            vocab_size - Size of the vocabulary of words\n",
        "            model_dim - Hidden dimensionality to use inside the Transformer\n",
        "            num_classes - Number of classes to predict per sequence element\n",
        "            num_heads - Number of heads to use in the Multi-Head Attention blocks\n",
        "            num_layers - Number of encoder blocks to use.\n",
        "            lr - Learning rate in the optimizer\n",
        "            warmup - Number of warmup steps. Usually between 50 and 500\n",
        "            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
        "            pad_id - symbol to be ignored by the encoder\n",
        "            ignore_target - symbol to be ignored for loss calculation\n",
        "            dropout - Dropout to apply inside the model\n",
        "            input_dropout - Dropout to apply on the input features\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self._create_model()\n",
        "\n",
        "    def _create_model(self):\n",
        "        # The very first thing to do is to map input token ids to embedding vectors of size model_dim\n",
        "        # for that we can use an Embedding layer\n",
        "        self.embed = nn.Embedding(\n",
        "            num_embeddings=self.hparams.vocab_size, \n",
        "            embedding_dim=self.hparams.model_dim,\n",
        "            # we reserve a special symbol in the vocabulary for positions that\n",
        "            # should be ignored by the encoder\n",
        "            # this neat trick allows us to have sequences of variable length\n",
        "            # in an input tensor of fixed length\n",
        "            padding_idx=self.hparams.pad_id, \n",
        "        )\n",
        "        # Positional encoding for sequences\n",
        "        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n",
        "        # Transformer\n",
        "        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n",
        "                                              input_dim=self.hparams.model_dim,\n",
        "                                              dim_feedforward=2*self.hparams.model_dim,\n",
        "                                              num_heads=self.hparams.num_heads,\n",
        "                                              dropout=self.hparams.dropout)\n",
        "        # Output classifier per sequence lement\n",
        "        # For convenience, we will be predicting the Categorical parameter\n",
        "        # up to a softmax transformation. These are also known as logits.\n",
        "        # We will make sure to use the softmax transformation whenever we\n",
        "        # need to interpret the Transformer outputs as probabilities. \n",
        "        # This helps for numerical stability, since in many contexts \n",
        "        # logits are sufficient for the relevant computations\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
        "            nn.LayerNorm(self.hparams.model_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(self.hparams.dropout),\n",
        "            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n",
        "        ) \n",
        "\n",
        "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features of shape [Batch, SeqLen]\n",
        "            mask - Mask to apply on the attention outputs (optional)\n",
        "            add_positional_encoding - If True, we add the positional encoding to the input.\n",
        "                                      Might not be desired for some tasks.\n",
        "        \"\"\"\n",
        "        x = self.embed(x)\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        x = self.transformer(x, mask=mask)\n",
        "        x = self.output_net(x)\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"\n",
        "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
        "        Input arguments same as the forward pass.\n",
        "        \"\"\"\n",
        "        x = self.embed(x)\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
        "        return attention_maps\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "        \n",
        "        # Apply lr scheduler per step\n",
        "        lr_scheduler = CosineWarmupScheduler(optimizer, \n",
        "                                             warmup=self.hparams.warmup, \n",
        "                                             max_iters=self.hparams.max_iters)\n",
        "        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError    \n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-6aZWQEVn7J"
      },
      "source": [
        "## Experiment\n",
        "\n",
        "After having finished the implementation of the Transformer architecture, we can start experimenting and apply it to various tasks. In this notebook, we will focus on a sequence labelling task: part-of-speech tagging. \n",
        "\n",
        "A sequence labelling task is a special type of sequence-to-sequence task where  input _and_ the output are sequences with the exact same length. For each step of the input sequence, the output sequence provides a label from a finite set of known categories. \n",
        "\n",
        "Let's create data loaders for our tagged corpus:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TaggedCorpus(training_x, training_y, word_vocab, tag_vocab)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=TaggedCorpus.pad_to_longest)\n",
        "\n",
        "val_dataset = TaggedCorpus(dev_x, dev_y, word_vocab, tag_vocab)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, collate_fn=TaggedCorpus.pad_to_longest)\n",
        "\n",
        "test_dataset = TaggedCorpus(test_x, test_y, word_vocab, tag_vocab)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=TaggedCorpus.pad_to_longest)"
      ],
      "metadata": {
        "id": "jDxMYsBiYIWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9si7xYcuVn7L"
      },
      "source": [
        "Let's look at an arbitrary sample of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBHA0BhgVn7L"
      },
      "outputs": [],
      "source": [
        "inp_data, labels = train_loader.dataset[0]\n",
        "print(\"Input data:\", inp_data)\n",
        "print(\"Labels:    \", labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZpxS3mYVn7L"
      },
      "source": [
        "During training, we pass the input sequence through the Transformer encoder and predict the logits that parameterise the distribution of each output tag. We train this architecture via maximum likelihood estimation (a stochastic gradient-based approximation of it). This means our loss is the negative of an estimate of the log-likelihood function, further normalised by number of tokens (which helps with gradient stability). \n",
        "\n",
        "To implement the training dynamic, we create a new class inheriting from `TransformerPredictor` and overwriting the training, validation and test step functions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import Categorical\n",
        "\n",
        "\n",
        "class TaggerPredictor(TransformerPredictor):\n",
        "    \n",
        "    def _calculate_loss(self, batch, mode=\"train\"):\n",
        "        # Fetch data and transform categories to one-hot vectors\n",
        "        inp_data, labels = batch\n",
        "        #inp_data = F.one_hot(inp_data, num_classes=self.hparams.num_classes).float()\n",
        "        \n",
        "        # Perform prediction and calculate loss and accuracy\n",
        "        logits = self.forward(inp_data, add_positional_encoding=True)\n",
        "        # when we construct the Categorical object we can give it `probs` \n",
        "        # or `logits`, we will give it `logits` so that it can perform \n",
        "        # numerically optimised computations whenever probs or log_probs are needed.\n",
        "        # [batch_size, max_len]\n",
        "        log_probs = Categorical(logits=logits).log_prob(labels)        \n",
        "        # The loss is the negative log likelihood\n",
        "        # masked to discard positions that are padded\n",
        "        # and normalised by number of tokens (which does not affect the optimum \n",
        "        # but makes the gradient norm more stable, which is useful for optimisation)\n",
        "        avg_log_likelihood = torch.where(\n",
        "            labels == self.hparams.ignore_target, \n",
        "            torch.zeros_like(log_probs),\n",
        "            log_probs\n",
        "        ).mean()\n",
        "        loss = - avg_log_likelihood\n",
        "\n",
        "        # In deep learning papers and code, most people use the interpretation of \n",
        "        # the loss as a cross entropy. This is numerically equivalent to the MLE interpretation \n",
        "        # we give above. But requires conceptualising the observations as 'one-hot distributions'.\n",
        "        # There's nothing right or wrong about it. It's just another view. \n",
        "        # This is how you'd implement it in torch:\n",
        "        #loss = F.cross_entropy(logits.view(-1,logits.size(-1)), labels.view(-1), ignore_index=self.hparams.ignore_target)\n",
        "        \n",
        "        # Let's make independent argmax predictions for the labels:\n",
        "        label_mask = labels != self.hparams.ignore_target\n",
        "        acc = ((logits.argmax(dim=-1) == labels).float() * label_mask.float()).sum() / label_mask.float().sum()        \n",
        "        \n",
        "        # Logging\n",
        "        self.log(f\"{mode}_loss\", loss)\n",
        "        self.log(f\"{mode}_acc\", acc)\n",
        "        self.log(f\"{mode}_ppl\", torch.exp(- avg_log_likelihood))\n",
        "        return loss, acc\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"val\")\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"test\")"
      ],
      "metadata": {
        "id": "U_exCx8lYuuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBQjYADZVn7M"
      },
      "source": [
        "Finally, we can create a training function similar to the one we have seen in Tutorial 5 for PyTorch Lightning. We create a `pl.Trainer` object, running for $N$ epochs, logging in TensorBoard, and saving our best model based on the validation. Afterward, we test our models on the test set. An additional parameter we pass to the trainer here is `gradient_clip_val`. This clips the norm of the gradients for all parameters before taking an optimizer step and prevents the model from diverging if we obtain very high gradients at, for instance, sharp loss surfaces (see many good blog posts on gradient clipping, like [DeepAI glossary](https://deepai.org/machine-learning-glossary-and-terms/gradient-clipping)). For Transformers, gradient clipping can help to further stabilize the training during the first few iterations, and also afterward. In plain PyTorch, you can apply gradient clipping via `torch.nn.utils.clip_grad_norm_(...)` (see [documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_)). The clip value is usually between 0.5 and 10, depending on how harsh you want to clip large gradients. After having explained this, let's implement the training function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yR04h_KVn7M"
      },
      "outputs": [],
      "source": [
        "def train_tagger(**kwargs):\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"Tagger\")\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    trainer = pl.Trainer(\n",
        "        default_root_dir=root_dir, \n",
        "        callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "        accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
        "        devices=1,\n",
        "        max_epochs=10,\n",
        "        gradient_clip_val=5\n",
        "    )\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "    \n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"Tagger.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model, loading...\")\n",
        "        model = TaggerPredictor.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        model = TaggerPredictor(max_iters=trainer.max_epochs*len(train_loader), **kwargs)\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "        \n",
        "    # Test best model on validation and test set\n",
        "    val_result = trainer.test(model, val_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_loader, verbose=False)\n",
        "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n",
        "    \n",
        "    model = model.to(device)\n",
        "    return model, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJBSqe4vVn7N"
      },
      "source": [
        "Finally, we can train the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv5FJUJ6Vn7N"
      },
      "outputs": [],
      "source": [
        "tagger_model, tagger_result = train_tagger(\n",
        "    vocab_size=len(train_dataset.word_vocab),\n",
        "    model_dim=32, # on CPU you may need to lower this number\n",
        "    num_heads=4, # on CPU you may need to lower this number\n",
        "    num_classes=len(train_dataset.tag_vocab),\n",
        "    num_layers=2, # on CPU you may need to lower this number\n",
        "    dropout=0.0,\n",
        "    lr=5e-4,\n",
        "    warmup=50,\n",
        "    pad_id=train_dataset.word_vocab.pad_id,\n",
        "    ignore_target=train_dataset.tag_vocab.pad_id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA6PaR0lVn7N"
      },
      "source": [
        "The warning of PyTorch Lightning regarding the number of workers can be ignored for now. As the data set is so simple and the `__getitem__` finishes a neglectable time, we don't need subprocesses to provide us the data (in fact, more workers can slow down the training as we have communication overhead among processes/threads). First, let's print the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QLKCtiMVn7N"
      },
      "outputs": [],
      "source": [
        "print(f\"Val accuracy:  {(100.0 * tagger_result['val_acc']):4.2f}%\")\n",
        "print(f\"Test accuracy: {(100.0 * tagger_result['test_acc']):4.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BDd82KHVn7O"
      },
      "source": [
        "We can visualise the self-attention layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMSI7o0IVn7O"
      },
      "outputs": [],
      "source": [
        "data_input, labels = next(iter(val_loader))\n",
        "attention_maps = tagger_model.get_attention_maps(data_input.to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-kuekd8Vn7O"
      },
      "source": [
        "The object `attention_maps` is a list of length $N$ where $N$ is the number of layers. Each element is a tensor of shape [Batch, Heads, SeqLen, SeqLen], which we can verify below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzWf78O6Vn7O"
      },
      "outputs": [],
      "source": [
        "attention_maps[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFT7s6bLVn7O"
      },
      "source": [
        "Next, we will write a plotting function that takes as input the sequences, attention maps, and an index indicating for which batch element we want to visualize the attention map. We will create a plot where over rows, we have different layers, while over columns, we show the different heads. Remember that the softmax has been applied for each row separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thRF2JzxVn7O"
      },
      "outputs": [],
      "source": [
        "def plot_attention_maps(input_data, attn_maps, vocab, idx=0, pad_id=0, fig_size_multiplier=1.):\n",
        "    if input_data is not None:\n",
        "        input_data = input_data[idx].detach().cpu().numpy()\n",
        "    else:\n",
        "        input_data = np.arange(attn_maps[0][idx].shape[-1])\n",
        "    \n",
        "    # discard padding\n",
        "    seq_len = (input_data != pad_id).sum()\n",
        "    attn_maps = [m[idx].detach().cpu().numpy()[...,:seq_len,:seq_len] for m in attn_maps]\n",
        "    \n",
        "    num_heads = attn_maps[0].shape[0]\n",
        "    num_layers = len(attn_maps)\n",
        "    \n",
        "    fig_size = 4 if num_heads == 1 else 3\n",
        "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size*fig_size_multiplier, num_layers*fig_size*fig_size_multiplier))\n",
        "    if num_layers == 1:\n",
        "        ax = [ax]\n",
        "    if num_heads == 1:\n",
        "        ax = [[a] for a in ax]\n",
        "    \n",
        "    tokens = vocab.decode(input_data.tolist())\n",
        "\n",
        "    for row in range(num_layers):\n",
        "        for column in range(num_heads):\n",
        "            ax[row][column].imshow(attn_maps[row][column], origin='lower', vmin=0)\n",
        "            ax[row][column].set_xticks(list(range(seq_len)))\n",
        "            ax[row][column].set_xticklabels(tokens, rotation=90, fontsize=8)\n",
        "            ax[row][column].set_yticks(list(range(seq_len)))\n",
        "            ax[row][column].set_yticklabels(tokens, fontsize=8)\n",
        "            ax[row][column].invert_yaxis()\n",
        "            ax[row][column].set_title(f\"Layer {row+1}, Head {column+1}\")\n",
        "    fig.tight_layout(h_pad=0.5, w_pad=0.5)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv67Q9BgVn7O"
      },
      "source": [
        "Finally, we can plot the attention map of our trained Transformer on the POS tagging task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt6T6eMDVn7O"
      },
      "outputs": [],
      "source": [
        "plot_attention_maps(data_input, attention_maps, train_dataset.word_vocab, idx=0, pad_id=train_dataset.word_vocab.pad_id, fig_size_multiplier=1.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_P5QAN9Vn7S"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we took a closer look at the Multi-Head Attention layer which uses a scaled dot product between queries and keys to find correlations and similarities between input elements. The Transformer architecture is based on the Multi-Head Attention layer and applies multiple of them in a ResNet-like block. The Transformer is a very important, recent architecture that can be applied to many tasks and datasets. Although it is best known for its success in NLP, there is so much more to it. We have seen its application on a sequence labelling tasks. Its property of being permutation-equivariant if we do not provide any positional encodings, allows it to generalize to many settings. Hence, it is important to know the architecture, but also its possible issues such as the gradient problem during the first iterations solved by learning rate warm-up. If you are interested in continuing with the study of the Transformer architecture, please have a look at the blog posts listed at the beginning of the tutorial notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAbixQJ_Vn7S"
      },
      "source": [
        "---\n",
        "\n",
        "[![Star our repository](https://img.shields.io/static/v1.svg?logo=star&label=&message=Star%20Our%20Repository&color=yellow)](https://github.com/phlippe/uvadlc_notebooks/)  If you found this tutorial helpful, consider -ing our repository.    \n",
        "[![Ask questions](https://img.shields.io/static/v1.svg?logo=star&label=&message=Ask%20Questions&color=9cf)](https://github.com/phlippe/uvadlc_notebooks/issues)  For any questions, typos, or bugs that you found, please raise an issue on GitHub. \n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}